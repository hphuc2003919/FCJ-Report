[{"uri":"https://hphuc2003919.github.io/FCJ-Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/5.3-infrastructure-setup/5.3.1-deploy/","title":"Deploy Infrastructure","tags":[],"description":"","content":"Source: https://github.com/Aohk22/fcj-1-file-analyzer/tree/main\nDeployment Steps Run the Terraform workflow and Image Builder pipeline in order:\nterraform init terraform fmt terraform validate Create the Image Builder resources:\n./plan_imagebuilder.sh terraform apply plan.tfplan Script source:\n#!/usr/bin/env sh terraform plan -out=img_builder.tfplan \\ -target=aws_imagebuilder_infrastructure_configuration.imgbuilder_infra_config \\ -target=module.image_builder_pipeline_fhandle \\ -target=module.image_builder_pipeline_fquery \\ -target=module.instance_profile_imgbuilder \\ -target=aws_internet_gateway.igw_main \\ -target=aws_route_table.route_table_public \\ -target=aws_route_table_association.rt_associate_sn_pub_1 \\ -target=aws_iam_role_policy_attachment.imgbuilder_ec2_profile \\ -target=aws_iam_role_policy_attachment.imgbuilder_ec2_ecr_profile \\ -target=aws_iam_role_policy_attachment.imgbuilder_ssm_core Build the AMIs:\n./start_image_pipeline.sh # wait until AMI build completes Script source, it polls the AWS api for status:\n#!/usr/bin/env bash arn_build_versions=() arn_pipelines=( \u0026#34;arn:aws:imagebuilder:ap-southeast-2:005716755011:image-pipeline/fhandle-img-pipeline\u0026#34; \u0026#34;arn:aws:imagebuilder:ap-southeast-2:005716755011:image-pipeline/fquery-img-pipeline\u0026#34; ) echo \u0026#34;Sending pipeline requests.\u0026#34; for arn in \u0026#34;${arn_pipelines[@]}\u0026#34;; do result=$( aws imagebuilder start-image-pipeline-execution \\ --image-pipeline-arn \u0026#34;${arn}\u0026#34; ) arn_build_versions+=(\u0026#34;$(echo \u0026#34;${result}\u0026#34; | jq -r \u0026#39;.imageBuildVersionArn\u0026#39;)\u0026#34;) done for arn in \u0026#34;${arn_build_versions[@]}\u0026#34;; do echo \u0026#34;$arn\u0026#34; done # wait for image. for build_version in \u0026#34;${arn_build_versions[@]}\u0026#34;; do echo \u0026#34;Polling ${build_version}...\u0026#34; prev_status=\u0026#34;_none\u0026#34; while status=\u0026#34;$(aws imagebuilder get-image --image-build-version-arn \u0026#34;$build_version\u0026#34; | jq -r \u0026#39;.image.state.status\u0026#39;)\u0026#34; if [[ \u0026#34;$prev_status\u0026#34; != \u0026#34;${status}\u0026#34; ]]; then echo \u0026#34;Status: ${status}\u0026#34; prev_status=\u0026#34;${status}\u0026#34; fi sleep 5 [[ \u0026#34;$status\u0026#34; != \u0026#34;AVAILABLE\u0026#34; ]] do true; done done echo \u0026#34;Image build complete.\u0026#34; Deploy the full infrastructure:\nterraform plan -out=plan.tfplan terraform apply plan.tfplan "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders (Track 1: Gen AI and Data)” Event Objectives Explore the latest AWS strategies and services for generative AI and data-driven workloads Provide industry insights from leading executives and technology leaders Share best practices for building a unified, scalable data foundation on AWS Introduce modern approaches like the AI-Driven Development Lifecycle (AI-DLC) Highlight security fundamentals and the evolving role of AI agents in productivity Speakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – VP, General Manager Asia Pacific \u0026amp; Japan, AWS Jun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist SA, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Key Highlights Opening and Keynotes Government representatives emphasized Vietnam’s digital transformation journey. AWS executives shared cloud adoption strategies across Asia Pacific. Customer stories from Techcombank and U2U Network highlighted real-world GenAI adoption. Panel Discussion: Navigating the GenAI Revolution Leaders from ELSA Corp, Nexttech Group, and TymeX discussed executive strategies for innovation, AI integration, and organizational change. Breakout Tracks (Track 1: GenAI and Data) Building a Unified Data Foundation on AWS – Strategies for ingestion, storage, processing, and governance to enable AI/analytics workloads. GenAI Adoption and Roadmap – AWS’s vision for empowering organizations to adopt and scale GenAI solutions. AI-Driven Development Lifecycle (AI-DLC) – AI as a co-developer across the entire software development lifecycle. Securing Generative AI Applications – Best practices for encryption, zero-trust, monitoring, and access control in GenAI stacks. Beyond Automation: AI Agents – The shift from simple automation to autonomous AI agents that adapt, learn, and multiply productivity. Key Takeaways Strategic Insights GenAI is moving beyond experimentation into enterprise-scale adoption. Executive leadership must foster innovation cultures while aligning AI initiatives with business objectives. Technical Learning Importance of building scalable data foundations with governance for AI workloads. AI-DLC introduces a new paradigm: AI as a collaborator, not just a tool. Security is multi-layered: infrastructure, model, and application levels. Future Trends AI agents will act as productivity partners, not just automation tools. Cloud-native services provide the backbone for scalable and secure GenAI adoption. Applying to Work Apply data foundation strategies to improve data governance and scalability in current projects. Explore AI-DLC concepts to accelerate development while maintaining human oversight. Adopt security-first approaches when designing AI-powered systems. Investigate potential applications of AI agents for productivity gains in workflows. Event Experience Attending Vietnam Cloud Day 2025 was an enriching experience, offering both strategic and technical insights. The mix of keynote sessions, executive panel discussions, and deep-dive breakout tracks allowed me to:\nLearn directly from AWS experts and industry leaders. Understand how Vietnamese enterprises like Techcombank are implementing GenAI. Gain practical knowledge of securing and scaling GenAI workloads. Network with professionals and peers in the cloud and AI community. Lessons learned Building a robust data foundation is critical before scaling GenAI initiatives. Executive alignment and organizational culture play key roles in AI adoption success. AI-DLC and AI agents will reshape the way software development and business processes operate. Security cannot be an afterthought—it must be embedded into every layer of the GenAI stack. Overall, attending Vietnam Cloud Day 2025 gave me valuable insights into generative AI, data foundations, cloud security, and the future role of AI in driving innovation and productivity.\n"},{"uri":"https://hphuc2003919.github.io/FCJ-Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS” Event Objectives Introduce participants to core Generative AI concepts and AWS AI/ML services Provide practical, hands-on guidance for building GenAI applications with Amazon Bedrock Demonstrate prompt engineering, RAG architecture, and AI agent workflow design Highlight best practices for safety, content moderation, and responsible AI Speakers Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Key Highlights Foundation Models (FMs) Overview Compared Claude, Llama, and Amazon Titan for GenAI use cases Learned criteria for selecting the right foundation model for specific tasks Prompt Engineering Explored chain-of-thought reasoning and few-shot prompting Learned optimization techniques to improve AI output quality and reliability Retrieval-Augmented Generation (RAG) Understood RAG architecture and integrating enterprise knowledge bases Reviewed real-world use cases combining LLM reasoning with structured knowledge Bedrock Agents Learned how to orchestrate multi-step AI workflows Integrated tools and components to automate complex AI tasks Safety \u0026amp; Guardrails Implemented safety measures, content filtering, and secure output generation Emphasized responsible AI deployment in production environments Hands-On Demo Built a generative AI chatbot using Amazon Bedrock Applied prompt engineering, RAG patterns, and agent workflows in practice Key Takeaways Effective AI solutions begin with selecting the right foundation model for the task Prompt engineering significantly improves output quality and reliability RAG combines LLM reasoning with enterprise knowledge for more accurate responses Bedrock Agents enable automated multi-step workflows for advanced AI applications Safety and guardrails are essential for deploying AI responsibly in production AWS provides a comprehensive ecosystem to build, scale, and secure GenAI workloads Applying to Work Apply prompt engineering and RAG patterns to improve internal AI tools or prototypes Use Bedrock Agents to automate workflow tasks in future projects Integrate guardrails into GenAI applications to ensure safe, compliant usage Experiment with building chatbots or assistants using Bedrock components Event Experience Attending the event provided hands-on exposure to Generative AI implementation on AWS. The live demonstrations and expert explanations deepened my understanding of Bedrock capabilities, enhanced my practical skills in prompt engineering and RAG design, and boosted my confidence in building secure and effective GenAI applications.\nLesson Learned Selecting the right foundation model (FM) is critical for effective AI solutions. Prompt engineering significantly improves output quality, reliability, and reasoning in GenAI applications. RAG architecture enables combining LLM reasoning with enterprise knowledge for more accurate responses. Bedrock Agents facilitate automation of multi-step workflows and advanced AI tasks. Implementing safety guardrails is essential for responsible AI deployment. Hands-on practice deepens understanding of GenAI workflows and AWS services for building scalable AI solutions. Some event photos Overall, attending the event gave me practical, hands-on insights into building GenAI applications using Amazon Bedrock, including foundation model selection, prompt engineering, RAG design, AI agents, and implementing safety guardrails.\n"},{"uri":"https://hphuc2003919.github.io/FCJ-Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #2 – DevOps on AWS” Event Objectives Provide a comprehensive understanding of DevOps culture, principles, and best practices Demonstrate how to implement CI/CD pipelines using AWS Developer Tools Introduce Infrastructure as Code (IaC) solutions with CloudFormation and AWS CDK Explore containerization, orchestration, and microservices deployment on AWS Strengthen monitoring and observability skills using CloudWatch and X-Ray Guide attendees on DevOps career development and AWS certifications Key Highlights DevOps Mindset: Collaboration, continuous improvement, DORA metrics (MTTR, deployment frequency), and culture-building CI/CD Pipeline on AWS: Version control with CodeCommit \u0026amp; Git strategies CodeBuild for compiling and testing CodeDeploy with blue/green, canary, and rolling deployments CodePipeline for full automation Live demo of an end-to-end CI/CD workflow Infrastructure as Code (IaC): CloudFormation stacks, templates, drift detection AWS CDK constructs, reusable patterns, and multi-language support Demo comparing IaC tools and selecting the right approach Containers \u0026amp; Microservices: Docker fundamentals and container lifecycle Amazon ECR for image storage and scanning Amazon ECS \u0026amp; EKS for orchestration and scaling AWS App Runner for simplified deployments Case study on microservices deployment patterns Monitoring \u0026amp; Observability: CloudWatch logs, metrics, alarms, dashboards AWS X-Ray for distributed tracing and root cause analysis Best practices for alerting and on-call processes DevOps Best Practices: Feature flags, automated testing, A/B testing Incident response, postmortems, and operational excellence Real-world DevOps transformation stories Key Takeaways DevOps is as much about culture and communication as it is about tools CI/CD automation improves velocity, reliability, and reduces deployment risk IaC is essential for consistent, repeatable, and scalable infrastructure management Containers and orchestrators (ECS/EKS) enable flexible microservices architectures Observability is critical for diagnosing performance issues and ensuring system health Modern deployment strategies (blue/green, canary, feature flags) reduce downtime and improve safety Applying to Work Implement CI/CD pipelines to automate builds, tests, and deployments Use CloudFormation or CDK to enforce infrastructure consistency across environments Adopt containerization for modular, scalable service design Apply monitoring dashboards and distributed tracing to improve operational visibility Introduce DevOps best practices such as automated testing, postmortems, and continuous improvement cycles Event Experience The session provided a complete, end-to-end view of how DevOps is practiced on AWS—from culture and metrics to pipelines, IaC, container deployments, and observability. The hands-on demos and real-world case studies helped clarify how modern engineering teams achieve high velocity, reliability, and operational excellence. The event significantly enhanced my confidence in applying DevOps principles and AWS tools to real projects.\nLesson Learned Infrastructure as Code (IaC) is essential: Using CloudFormation and CDK ensures repeatable, version-controlled infrastructure deployments. CI/CD automation improves reliability. Monitoring \u0026amp; Observability are critical: Tools like CloudWatch, CloudTrail, and X-Ray provide insights into system performance and help quickly detect failures. Integrating security checks, IAM policies, and guardrails into CI/CD pipelines strengthens security posture. Cross-functional teamwork between developers and operations ensures faster delivery and more stable systems. Scripting deployments, rollback mechanisms, and automated testing improve efficiency and reduce downtime. Using ECS, EKS, and Docker allows consistent environments, scaling, and deployment flexibility. Some event photos Overall, the event provided a full-day, hands-on learning experience covering CI/CD, Infrastructure as Code, container technologies, and observability best practices, helping me strengthen my DevOps mindset, technical skills, and understanding of AWS DevOps tooling.\n"},{"uri":"https://hphuc2003919.github.io/FCJ-Report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #3 – AWS Well-Architected Security Pillar” Event Objectives Introduce the five pillars of the AWS Well-Architected Security Framework Provide practical guidance on implementing modern IAM architectures Demonstrate detection and continuous monitoring with AWS-native tools Strengthen understanding of infrastructure and network security on AWS Deep dive into encryption, key management, and secrets management Walk through incident response scenarios with automation-driven remediation Key Highlights Pillar 1: Identity \u0026amp; Access Management (IAM) Modern IAM architecture: Users, Roles, Policies IAM Identity Center, permission sets, SCPs, permission boundaries Hands-on demo: validate IAM policies \u0026amp; simulate access Pillar 2: Detection Continuous monitoring using CloudTrail, GuardDuty, Security Hub Multi-layer logging: VPC Flow Logs, ALB/S3 logs Alerting \u0026amp; automation with EventBridge Detection-as-Code concept Pillar 3: Infrastructure Protection Network and workload security: VPC segmentation, Security Groups vs NACLs Protection via WAF, Shield, Network Firewall EC, ECS/EKS security basics Pillar 4: Data Protection Key management and KMS best practices: key rotation, grants Encryption at-rest \u0026amp; in-transit for S3, EBS, RDS, DynamoDB Secrets management: Secrets Manager \u0026amp; Parameter Store Data classification \u0026amp; access guardrails Pillar 5: Incident Response AWS IR lifecycle: preparation → detection → containment → recovery → lessons learned Playbooks: compromised IAM keys, S3 exposure, EC2 malware Automated incident response using Lambda \u0026amp; Step Functions Key Takeaways Strategic Insights Security Pillar is foundational to every AWS architecture, regardless of workload type Multi-account governance and Identity Center simplify large-scale access control Organizations must move beyond manual monitoring toward automation and Detection-as-Code Technical Learning IAM must enforce least privilege, strong isolation, and avoid long-term static credentials Continuous monitoring through CloudTrail, GuardDuty, and Security Hub strengthens visibility Proper segmentation and layered protections significantly reduce blast radius Strong data protection includes encryption, key policies, secrets rotation, and classification Automated IR workflows minimize response time and reduce operational overhead Applying to Work Use IAM Identity Center and SCPs to strengthen identity governance in multi-account environments Implement GuardDuty, Security Hub, and EventBridge rules to improve detection and alert automation Revisit VPC designs to ensure proper segmentation and least-privilege network access Apply encryption and secrets rotation best practices to existing workloads Build IR playbooks and automate critical steps with Lambda for faster containment Event Experience The event provided a highly practical, hands-on view of AWS security practices. The combination of conceptual explanations, architectural guidance, and live demonstrations gave me clearer insight into how AWS applies security at scale. Engaging discussions on Vietnam’s cloud security challenges made the content especially relevant to local organizations. The structured breakdown of each pillar helped reinforce the end-to-end perspective needed for real-world security architecture.\nLesson Learned IAM maturity is the first and most important step in building secure cloud environments Detection must be proactive, automated, and integrated across all layers of infrastructure Network segmentation and layered defense significantly reduce risk exposure Proper key and secrets management prevents many common security failures Incident response requires planning, automation, and continuous improvement Security is not a one-time task—it\u0026rsquo;s a continuous lifecycle across all workloads Overall, the event provided valuable hands-on learning and meaningful industry exposure, allowing me to deepen my understanding of cloud technologies while gaining practical skills through real-world demonstrations and discussions. It also strengthened my ability to learn independently, collaborate effectively, and stay updated with current trends—contributing positively to my professional growth in the IT and cloud domain.\n"},{"uri":"https://hphuc2003919.github.io/FCJ-Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Pham Quoc Huy Phuc\nPhone Number: 0942769280\nEmail: pqhuyphuc2003@gmail.com\nUniversity: FPT University\nMajor: Information Assurance\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 30/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand the concept of Cloud Computing and its benefits. Learn the key differences and advantages of AWS. Explore how to get started with AWS, from account creation to service management. Become familiar with AWS Global Infrastructure and management tools (Console, CLI, SDKs). Learn about cost optimization, creating budgets, and working with AWS Support. Practice through hands-on labs: account creation, security, IAM, budgets, and support. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Read and take note of FCJ program policies and guidelines. 08/09/2025 08/09/2025 https://policies.fcjuni.com/ - Learn about AWS and its main service groups: + Compute + Storage + Networking + Database + Security + \u0026hellip; 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 01-01: What is Cloud Computing? - Module 01-02: What Makes AWS Different? - Module 01-03: How to Start Your Cloud Journey 10/09/2025 10/09/2025 https://www.youtube.com/@AWSStudyGroup - Module 01-04: AWS Global Infrastructure - Module 01-05: AWS Service Management Tools - Module 01-06: Optimizing Costs \u0026amp; Working with AWS Support 11/09/2025 11/09/2025 https://www.youtube.com/@AWSStudyGroup - Hands-on Labs: + Create an AWS account + Setup Virtual MFA Device + Create admin group \u0026amp; admin user + Account authentication support 12/09/2025 12/09/2025 https://000001.awsstudygroup.com/ - Hands-on Labs: + Create Budgets (Cost, Usage, RI, Savings Plans) + Clean Up Budgets 13/09/2025 13/09/2025 https://000007.awsstudygroup.com/ - Hands-on Labs: + AWS Support Packages + Types of support requests + Change support package + Manage support requests 14/09/2025 14/09/2025 https://000009.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking \u0026amp; Content Delivery Database Security \u0026amp; Identity Analytics Machine Learning Theory Learned:\nUnderstood the concept of cloud computing and its advantages. Learned what makes AWS unique. Explored how to get started with AWS (account setup, IAM, service management). Gained knowledge of AWS Global Infrastructure (Regions, Availability Zones, Edge Locations). Became familiar with AWS Management Console, AWS CLI, and SDKs. Learned about cost optimization with Budgets and cost management tools. Explored AWS Support offerings and support request workflows. Hands-on Labs Completed:\nCreated and configured an AWS Free Tier account. Enabled MFA for improved account security. Created an Admin Group and Admin User using IAM. Verified account authentication and support setup. Created multiple types of Budgets (Cost, Usage, Reserved Instance, Savings Plans). Cleaned up budgets to prevent unnecessary costs. Explored and managed AWS Support Packages. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS Virtual Private Cloud (VPC) concepts, security, and multi-VPC features. Explore hybrid connectivity options including VPN and DirectConnect. Learn about Load Balancers and traffic management. Participate in Vietnam Cloud Day 2025 for exposure to real-world cloud applications. Complete hands-on labs to reinforce practical VPC and hybrid networking skills. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Module 02-01: AWS Virtual Private Cloud (VPC) concepts and architecture. - Module 02-02: VPC Security and Multi-VPC features (Security Groups, NACLs, VPC Peering, Transit Gateway). - Module 02-03: VPN, DirectConnect, Load Balancers, Extra Resources. 16/09/2025 16/09/2025 https://www.youtube.com/@AWSStudyGroup - Hands-on Lab: Amazon VPC and AWS Site-to-Site VPN Workshop 17/09/2025 17/09/2025 https://000003.awsstudygroup.com/ - Participate in Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders 18/09/2025 18/09/2025 - Hands-on Lab: Set up Hybrid DNS with Route 53 Resolver 20/09/2025 20/09/2025 https://000010.awsstudygroup.com/ Week 2 Achievements: Theory Learned:\nDeepened understanding of VPC architecture, including subnets, route tables, and gateways. Explored VPC security features: Security Groups and Network ACLs. Learned multi-VPC connectivity and management (VPC Peering, Transit Gateway). Understood hybrid network connections: VPN and DirectConnect. Learned about Load Balancer types and use cases. Hands-on Labs Completed:\nConfigured Amazon VPC and Site-to-Site VPN connections. Implemented Hybrid DNS with Route 53 Resolver for cross-network communication. Event Participation:\nAttended Vietnam Cloud Day 2025 to gain insights into GenAI, AWS infrastructure, and enterprise cloud adoption. Skills Gained:\nAWS networking fundamentals and VPC management. Security configuration for cloud resources. Hybrid connectivity setup and DNS management. Awareness of real-world cloud solutions and best practices from industry leaders. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Brainstorm and finalize a project idea for the internship program. Assign roles and responsibilities within the project team. Perform initial planning for project scope, tasks, and milestones. Gain practical experience with AWS networking labs: VPC Peering and Transit Gateway setup. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Brainstorm project ideas and decide on the project concept 22/09/2025 22/09/2025 - Assign roles and responsibilities for the project 23/09/2025 23/09/2025 - Initial project planning: define scope, tasks, and milestones 24/09/2025 24/09/2025 - Hands-on Lab: Setting up VPC Peering between AWS accounts 25/09/2025 25/09/2025 https://000019.awsstudygroup.com/ - Hands-on Lab: Set up AWS Transit Gateway for multi-VPC connectivity 26/09/2025 26/09/2025 https://000020.awsstudygroup.com/ Week 3 Achievements: Project Planning \u0026amp; Teamwork:\nFinalized the project idea. Defined my role as frontend developer and coordinated tasks with teammates. Completed initial planning, outlining project requirements, workflow, and timeline. Hands-on Labs Completed:\nConfigured VPC Peering for secure communication between multiple AWS VPCs. Implemented AWS Transit Gateway to manage and scale multi-VPC connectivity efficiently. Skills Gained:\nProject planning and team collaboration skills. Experience with AWS networking features: VPC Peering and Transit Gateway. Improved understanding of multi-account architecture and connectivity in AWS. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Develop the frontend service for the group project. Strengthen understanding of AWS Compute services, focusing on EC2, storage options, and autoscaling. Explore additional compute-related services: EFS, FSx, Lightsail, and MGN. Complete hands-on labs on AWS Backup, Storage Gateway, and Amazon S3. Start translating blogs. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Develop the frontend code for the group project 29/09/2025 03/10/2025 - Module 03-01 – Compute VM on AWS: + EC2 Instance Types + AMI / Backup / Key Pair + EBS + Instance Store + User Data + Meta Data + EC2 Auto Scaling 29/09/2025 29/09/2025 https://www.youtube.com/@AWSStudyGroup - Module 03-02 – EC2 Autoscaling, EFS, FSx, Lightsail, MGN - Hands-on Labs: + Deploy AWS Backup to the System + Using File Storage Gateway 30/09/2025 30/09/2025 - https://cloudjourney.awsstudygroup.com/ - https://000013.awsstudygroup.com/ - https://000024.awsstudygroup.com/ - Hands-on Lab: Starting with Amazon S3 01/10/2025 02/10/2025 https://000057.awsstudygroup.com/ - Translating blogs 03/10/2025 Week 4 Achievements: Project Development: Spent the entire week working on frontend implementation for the group project. Improved UI structure, layout, and basic component workflows. Collaborated with team members to align frontend with backend requirements and API design. AWS Theory Learned: Gained an in-depth understanding of Amazon EC2, including: Instance types for different workloads. AMIs and backup strategies. Key pairs and instance authentication. EBS vs Instance Store and their use cases. User Data scripting for automation. Metadata for instance information retrieval. Learned about EC2 Auto Scaling, how it adapts to load changes automatically. Explored advanced compute-related services: EFS for scalable shared file storage. FSx for high-performance workloads. Lightsail for simplified cloud hosting. MGN for migrating servers to AWS. Hands-on Labs Completed: Deploy AWS Backup to the System → Learned how to configure backups for EC2, EBS, and other resources.\nUsing File Storage Gateway → Practiced hybrid storage integration and working with on-premises file systems.\nStarting with Amazon S3 → Understood S3 buckets, versioning, storage classes, encryption, and object lifecycle policies.\nOther Activities Began translating blogs. The translation is still in progress. Skills Gained: Technical Skills\nStrengthened frontend development skills: responsive UI, component design, and user interaction handling. Gained solid understanding of EC2, AMIs, EBS vs Instance Store, User Data, Metadata, and Auto Scaling. Learned additional compute/storage services: EFS, FSx, Lightsail, and AWS MGN. Completed hands-on labs using AWS Backup, File Storage Gateway, and Amazon S3 (bucket creation, versioning, lifecycle, encryption). Cloud Architecture Skills\nImproved ability to select suitable storage solutions (EBS/EFS/S3/FSx). Learned best practices for scalable and secure compute environments. Soft Skills\nImproved teamwork while developing the project frontend. Enhanced time management and problem-solving through lab and project tasks. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Complete and integrate frontend code with backend services (Processing Service and Query Service). Complete translating AWS-related blogs. Learn AWS storage services in depth: S3, Glacier, Snow Family, Storage Gateway, Backup, and FSx. Gain hands-on experience with VM Import/Export and Amazon FSx for Windows File Server. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Complete and integrate frontend code with Processing Service and Query Service 06/10/2025 08/10/2025 - Translate Blog 1, 2, and 3 06/10/2025 07/10/2025 - Module 04-01: AWS Storage Services Overview - Module 04-02: Amazon S3 – Access Points \u0026amp; Storage Classes - Module 04-03: S3 Static Website, CORS, Access Control, Object Keys \u0026amp; Performance, Glacier - Module 04-04: Snow Family, Storage Gateway, AWS Backup 08/10/2025 08/10/2025 https://www.youtube.com/@AWSStudyGroup - Hands-on Lab: VM Import/Export 09/10/2025 09/10/2025 https://000014.awsstudygroup.com/ - Hands-on Lab: Amazon FSx for Windows File Server 10/10/2025 11/10/2025 https://000025.awsstudygroup.com/ Week 5 Achievements: Completed and integrated the frontend code with backend Processing Service and Query Service, ensuring smooth data flow. Translated Blog 1, 2, and 3. Gained knowledge of Amazon S3 features including access points, storage classes, static website hosting, CORS, Glacier, and data management. Explored AWS Snow Family, Storage Gateway, and Backup for hybrid and cloud storage solutions. Completed hands-on labs for VM Import/Export and Amazon FSx for Windows File Server. Practiced managing file storage, migrating VMs, and configuring cloud-native storage solutions. Skills Gained: Frontend-backend integration and troubleshooting. Understanding AWS storage services architecture and use cases. Practical experience with VM migration and FSx deployment. Knowledge of storage security, access control, and performance optimization. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand the AWS Shared Responsibility Model and its implications for security. Learn AWS Identity \u0026amp; Access Management (IAM), Cognito, AWS Organizations, and Identity Center. Gain knowledge of AWS Key Management Service (KMS) and Security Hub. Perform hands-on labs for managing AWS resources, access control, and security monitoring. Explore practical cost optimization techniques for EC2 using Lambda. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Module 05-01: Shared Responsibility Model - Module 05-02: Amazon Identity and Access Management (IAM) - Module 05-03: Amazon Cognito - Module 05-04: AWS Organizations - Module 05-05: AWS Identity Center - Module 05-06: Amazon Key Management Service (KMS) - Module 05-07: AWS Security Hub - Module 05-08: Hands-on and Additional Research 13/10/2025 14/10/2025 https://www.youtube.com/@AWSStudyGroup - Hands-on Lab: + Get started with AWS Security Hub + Optimizing EC2 Costs with Lambda 16/10/2025 16/10/2025 - https://000018.awsstudygroup.com/ - https://000022.awsstudygroup.com/ - Hands-on Lab: Manage Resources Using Tags and Resource Groups 17/10/2025 17/10/2025 https://000027.awsstudygroup.com/ - Hands-on Lab: Manage Access to EC2 Services with Resource Tags through IAM Services 17/10/2025 18/10/2025 https://000028.awsstudygroup.com/ Week 6 Achievements: Learned the AWS Shared Responsibility Model and its importance in cloud security. Gained understanding of AWS IAM, Cognito, Organizations, and Identity Center for managing identities and permissions. Explored AWS KMS for encryption and secure key management. Completed hands-on labs using AWS Security Hub to monitor security compliance and risks. Practiced cost optimization for EC2 instances using Lambda automation. Learned how to manage AWS resources effectively with tags and resource groups. Applied IAM policies to control access to EC2 services using resource tags. Skills Gained: AWS identity and access management (IAM, Cognito, Identity Center, Organizations). Security monitoring and compliance with AWS Security Hub. Resource tagging and group management for organized access control. Cost optimization for EC2 using Lambda functions. Applying AWS KMS for encryption and secure key handling. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Strengthen understanding of AWS IAM advanced features and encryption. Practice security-focused labs including permission boundaries, IAM roles, and KMS. Begin midterm preparation by reviewing security architecture and access control concepts. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Hands-on Lab: + LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY + Encrypt at Rest with AWS KMS 20/10/2025 20/10/2025 - https://000030.awsstudygroup.com/ - https://000033.awsstudygroup.com/ - Hands-on Lab: IAM Role \u0026amp; Condition - Practice designing IAM roles for applications 22/10/2025 22/10/2025 https://000044.awsstudygroup.com/ - Hands-on Lab: Granting authorization for an application to access AWS services with an IAM role 23/10/2025 23/10/2025 https://000048.awsstudygroup.com/ - Review Secure Architectures: IAM, MFA, SCP, Security Groups, NACLs 24/10/2025 24/10/2025 - Review Secure Architectures: Encryption (KMS, TLS/ACM), GuardDuty, Shield, WAF, Secrets Manager 25/10/2025 25/10/2025 Week 7 Achievements: Learned how IAM Permission Boundaries control maximum privileges. Practiced encrypting data at rest with AWS KMS. Configured IAM roles and permissions for applications. Reviewed core secure architecture concepts in preparation for the midterm. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Complete midterm preparation for all AWS Well-Architected pillars. Review resilient, high-performing, and cost-optimized architectures. Practice core AWS services and integration concepts. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Review Resilient Architectures: Multi-AZ, Multi-Region, Disaster Recovery strategies 27/10/2025 27/10/2025 - Review Resilient Architectures: Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore 28/10/2025 28/10/2025 - Review High-Performing Architectures: EC2 Auto Scaling, Lambda, Fargate 29/10/2025 29/10/2025 - Review High-Performing Architectures: S3/EFS/EBS performance, caching, CloudFront, Global Accelerator 30/10/2025 30/10/2025 - Taking mid-term exam 31/10/2025 31/10/2025 Week 8 Achievements: Consolidated understanding of AWS Well-Architected Framework. Strengthened knowledge of resilient and high-performing architectures. Reviewed cost-optimization strategies and AWS service best practices. Completed midterm exam. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Review core database concepts and AWS database services. Learn and practice with Amazon RDS, Amazon Aurora, Redshift, Elasticache, and DynamoDB. Understand data lake architecture and cost/performance optimization using AWS Glue and Athena. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Module 06-01: Database Concepts review - Module 06-02: Amazon RDS \u0026amp; Amazon Aurora - Module 06-03: Redshift - Elasticache 03/11/2025 03/11/2025 https://www.youtube.com/@AWSStudyGroup - Hands-on Lab: Amazon Relational Database Service (Amazon RDS) 05/11/2025 05/11/2025 https://000005.awsstudygroup.com/ - Hands-on Lab: Data Lake on AWS 06/11/2025 06/11/2025 https://000035.awsstudygroup.com/ - Hands-on Lab: Amazon DynamoDB Immersion Day 06/11/2025 07/11/2025 https://000039.awsstudygroup.com/ - Hands-on Lab: Cost and performance analysis with AWS Glue and Amazon Athena 08/11/2025 08/11/2025 https://000040.awsstudygroup.com/ Week 9 Achievements: Reviewed database fundamentals and AWS relational and NoSQL services.\nGained practical experience with Amazon RDS, Aurora, Redshift, Elasticache, and DynamoDB.\nLearned about data lake design, ETL processes with AWS Glue, and querying/analyzing data using Amazon Athena.\nImproved ability to evaluate cost and performance trade-offs in AWS database solutions.\nSkills Gained:\nDatabase design and management on AWS. RDS, Aurora, Redshift, Elasticache configuration and usage. DynamoDB best practices and hands-on immersion. Data lake architecture and cost/performance analysis using Glue and Athena. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"During my internship with the First Cloud Journey (FCJ) program, I completed the program over a period of 12 weeks. The worklog reflects my weekly activities, learning experiences, and practical applications of AWS services and cloud-related tasks.\nWeek 1: Getting familiar with AWS, its global infrastructure, management tools, cost optimization, and completing basic hands-on labs.\nWeek 2: Studying core AWS networking concepts including VPC architecture, security layers, hybrid connectivity, and completing related hands-on labs.\nWeek 3: Brainstorm project idea, assign roles, plan the workflow, and practice advanced networking labs such as VPC Peering and Transit Gateway.\nWeek 4: Develop the project’s frontend while learning AWS compute and storage services through EC2, Autoscaling, and storage-focused hands-on labs.\nWeek 5: Complete frontend-backend integration, translate AWS blogs, and gain hands-on experience with S3, Glacier, Snow Family, Storage Gateway, Backup, VM Import/Export, and FSx.\nWeek 6: Learned AWS security and identity services, applied hands-on labs with Security Hub, IAM, and resource tagging, and optimized EC2 costs using Lambda automation.\nWeek 7: Practiced IAM advanced features, KMS encryption, and reviewed secure architecture concepts in preparation for the midterm.\nWeek 8: Reviewed resilient, high-performing, and cost-optimized architectures, practiced core AWS services, and completed the midterm exam.\nWeek 9: Reviewed AWS database services, practiced relational and NoSQL databases, explored data lake architectures, and performed cost and performance analysis.\nWeek 10: Worked with DynamoDB, built serverless data lakes, learned real-time analytics, ETL pipelines, and data visualization using AWS services.\nWeek 11: Attended AWS DevOps series event, explored hybrid cloud and serverless integration, and practiced cost optimization strategies on AWS.\nWeek 12: Reviewed AWS security best practices, implemented secure IAM architectures, and attended the AWS Security Pillar workshop to deepen practical knowledge in cloud security.\n"},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"Workshop: Deploy File Analyzer Infrastructure with Terraform Introduction In this hands-on workshop, you will deploy a complete file analyzer application infrastructure on AWS using Terraform. The infrastructure includes auto-scaling groups, load balancers, VPC endpoints, and S3 integration - all provisioned as Infrastructure as Code.\nArchitecture Overview The infrastructure consists of:\nVPC with public and private subnets across 2 availability zones Application Load Balancer (ALB) for traffic distribution 2 Auto Scaling Groups: Public ASG: Frontend/web tier Private ASG: Backend/processing tier S3 Gateway Endpoint for secure S3 access Security Groups for network isolation IAM Roles for EC2 permissions What You\u0026rsquo;ll Learn Initialize and configure Terraform for AWS Deploy multi-tier infrastructure using Terraform modules Configure auto-scaling groups and load balancers Set up VPC endpoints for secure AWS service access Verify and test deployed infrastructure Clean up resources efficiently Time Required Estimated Duration: 20 minutes\nWorkshop Flow Prerequisites (2 minutes) - Setup tools and credentials Infrastructure Setup (10 minutes) - Configure and deploy Verification (5 minutes) - Test the application Cleanup (3 minutes) - Remove all resources "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/5.3-infrastructure-setup/5.3.2-checking/","title":"Checking using AWS console","tags":[],"description":"","content":"Checking If Terraform Created Infrastructure Correctly VPC and routes Image builder pipelines EC2 status Load balancer rules Target group health "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites Before starting this workshop, ensure you have the following ready:\n1. AWS Account Active AWS account with administrator access Recommended: Use a sandbox/development account Ensure you\u0026rsquo;re in the Asia Pacific (Singapore) ap-southeast-1 region 2. Required Tools Terraform (version \u0026gt;= 1.0)\n# Check installation terraform version # Download from: https://www.terraform.io/downloads AWS CLI (version \u0026gt;= 2.0)\n# Check installation aws --version # Configure credentials aws configure Git\n# Clone the project git clone https://github.com/Aohk22/fcj-1-file-analyzer.git cd fcj-1-file-analyzer/terraform/envs/dev 3. AWS Credentials Configure your AWS credentials:\naws configure # AWS Access Key ID: [Your Access Key] # AWS Secret Access Key: [Your Secret Key] # Default region: ap-southeast-1 # Default output format: json Or set environment variables:\nexport AWS_ACCESS_KEY_ID=\u0026#34;your-access-key\u0026#34; export AWS_SECRET_ACCESS_KEY=\u0026#34;your-secret-key\u0026#34; export AWS_DEFAULT_REGION=\u0026#34;ap-southeast-1\u0026#34; 4. Required IAM Permissions Your AWS user/role needs permissions for:\nEC2 (create instances, security groups, launch templates) Auto Scaling (create ASGs, launch configurations) ELB (create ALB, target groups, listeners) VPC (create subnets, route tables, endpoints) IAM (create roles, policies) 5. Cost Considerations Estimated cost: ~$5-10 for 20 minutes\nEC2 instances: t3.micro (included in free tier if eligible) ALB: ~$0.025/hour Data transfer: minimal Tip: Ensure you complete the cleanup section to avoid unexpected charges!\n6. Verify Setup Run these commands to verify your setup:\n# Check Terraform terraform version # Check AWS credentials aws sts get-caller-identity # Check region aws configure get region "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Multi-Platform File Analysis System - VirusTotal Clone 1. Project Summary This project aims to develop and deploy a multi-platform file analysis system, operating similarly to VirusTotal. The system will allow users to upload suspicious files for scanning and analysis by multiple antivirus engines and various other analysis services. The primary goal is to provide a powerful, user-friendly tool for detecting and assessing potential file-based threats, thereby enhancing cybersecurity.\nThe system will be entirely built and deployed on the Amazon Web Services (AWS) cloud platform, leveraging AWS\u0026rsquo;s leading managed services, scalability, and security features to ensure high performance, reliability, and availability.\n2. Problem Statement The Challenge In an increasingly sophisticated and prevalent cybersecurity threat landscape, the demand for effective file analysis tools is paramount. VirusTotal has proven its value as a critical community service, helping users and cybersecurity professionals quickly identify malicious files. This project is initiated with the desire to create a similar, customizable, and extensible solution, serving research, educational, or specific application purposes.\nThe Solution This platform provides a centralized web interface where users can:\nUpload suspicious files (executables, scripts, office documents). Submit domains, IP addresses, or URLs for analysis. Automatically scan data with multiple antivirus engines, sandbox environments, and OSINT sources. Correlate results with threat intelligence feeds to improve detection accuracy. Share anonymized reports with the research community to foster collaboration. Analysis requests are processed in two different ways:\nFast Query (Query Service):\nIf the file/URL has already been analyzed, the Web Server sends its hash to the Query Service. The Query Service checks DynamoDB for existing results. If found, results are instantly returned to the user via the web interface. New Processing (Processing Service):\nIf the data is not yet available, the system forwards the raw file/URL to the Processing Service. The Processing Service performs in-depth analysis, scanning, and report generation. Results are sent back to the Web Server for user delivery and simultaneously stored in DynamoDB for future queries. This hybrid approach reduces response times for repeated requests while ensuring scalability for new analyses.\nObjectives Develop Core Functionality: Build a user interface (UI) that allows file uploads, a query service to check for existing analysis results, and a processing service to perform new file analyses. Multi-Engine Integration: Enable integration with various scanning and analysis tools (e.g., APIs of antivirus engines, static/dynamic analysis sandboxes). AWS Deployment: Design and deploy the system architecture on AWS to ensure high availability, flexible scalability, and robust security. Cost Optimization: Build a cost-effective architecture, leveraging appropriate AWS services for the project\u0026rsquo;s scale. Establish CI/CD Pipeline: Set up a Continuous Integration/Continuous Delivery (CI/CD) pipeline to automate application development and deployment. 3. Solution Architecture Summary User sends analysis request for file. Web Server sends request to Query Service. (Query using file hash) Query Service communicates with Database. Database responds to Query Service. Query Service responds to Web Server. If query successful send HTML response to User. If not then send file to Processing Service. (Web Server should hold raw file data) Processing Service sends report back to Web Server. Also stores report in Database. Database syncs. Diagram for AWS Web App Layer (UI):\nUsers access the system through the ALB, which distributes requests to EC2 instances in the Auto Scaling Group. This layer handles the interface and request intake.\nServices Layer:\nIncludes the Query Service and Processing Service, both deployed in Auto Scaling Groups within private subnets.\nQuery Service: Connects to DynamoDB to handle hash-based queries. Processing Service: Receives raw data, performs malware analysis, generates reports, and stores results in DynamoDB. Data Layer:\nAmazon DynamoDB stores two categories of data:\nView: Analysis results ready for user consumption. Event Store: Logs and raw data for deeper investigations. Scalability:\nBoth the Web App and Services layers use Auto Scaling Groups to ensure the system can handle high request volumes without compromising performance.\nAWS Services Used The system leverages the following key AWS services:\nAmazon VPC (Virtual Private Cloud): Creates an isolated virtual network, divided into multiple subnets (10.0.100.0/24, 10.0.101.0/24, 10.0.102.0/24, 10.0.103.0/24) across Availability Zones for high availability. Elastic Load Balancer (ALB): Distributes incoming traffic to Web App (UI) instances running on EC2. Amazon EC2: Hosts the Web App and backend services. Auto Scaling Group: Dynamically scales the number of EC2 instances to maintain performance and cost efficiency. Amazon DynamoDB: A NoSQL database that stores analysis reports, query results, and service synchronization data. 5. Roadmap \u0026amp; Development Milestones Project Plan: During internship (Months 1–3): 3 months in total. Month 1: Research AWS and upgrade hardware. Month 2: Design and refine the system architecture. Month 3: Deploy, test, and launch the system. 6. Budget Estimation Infrastructure Costs AWS Services: EC2 instances:\t6 × t4g.nano (6 × 0.0042 × 720h): $18.14 EBS\t6 × 20 GB = 120 GB × $0.10/GB:\t$12.00 Load Balancer (ALB)\t1 ALB, light traffic:\t$15.00 NAT instance\t1 × t4g.nano (replacement NAT Gateway):\t$3.02 DynamoDB\t10 GB, small on-demand:\t$5.00 Data transfer OUT\t100 GB × $0.09/GB:\t$9.00 CloudWatch \u0026amp; misc\tLogs + basicmetrics:\t$3.00 Total\t≈ $69.14 / month\n7. Risk Assessment Risk Matrix Malware Infection: High impact, low probability. API Rate Limiting: High impact, medium probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Malware: Strict network isolation (Private Subnets) and non-execution policies. API: Caching analysis results in DynamoDB to minimize external calls. Cost: AWS Budget alerts and Auto Scaling limits. Contingency Plans Switch to \u0026ldquo;Cached Only\u0026rdquo; mode if external APIs fail. Rapid infrastructure teardown via Terraform if costs spike. 8. Expected Outcomes Technical Improvements Automated scanning workflow replaces manual checks. High availability achieved through AWS Auto Scaling. Long-term Value Centralized threat database for instant hash lookups. Reusable Terraform modules for future cloud projects. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Gain hands-on experience with Amazon DynamoDB and building serverless data lakes. Learn to design and implement analytics pipelines using AWS services including Kinesis, Glue, EMR, Athena, and QuickSight. Explore real-time data streaming, transformation, cataloging, and visualization. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Hands-on Labs: + Work with Amazon DynamoDB + Build a data lake with your data 11/11/2025 11/11/2025 - https://000060.awsstudygroup.com/ - https://000070.awsstudygroup.com/ - Hands-on Lab: Analytics on AWS Workshop 12/11/2025 12/11/2025 https://000072.awsstudygroup.com/ - Hands-on Lab: Get started with Amazon QuickSight 14/11/2025 14/11/2025 https://000073.awsstudygroup.com/ - Attend AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS 15/11/2025 15/11/2025 Week 10 Achievements: Practiced working with Amazon DynamoDB and designing serverless data lakes.\nBuilt and managed data processing pipelines using S3, Kinesis, Glue, EMR, and Redshift.\nLearned how to perform data transformation, cataloging, and querying effectively.\nGained hands-on experience in data visualization using Amazon QuickSight.\nStrengthened understanding of real-time streaming and analytics workflows on AWS.\nSkills Gained:\nDynamoDB design and operations. Building serverless data lakes and ETL pipelines. Real-time data streaming and analysis with Kinesis. Using AWS Glue for ETL, transformation, and cataloging. Querying and visualizing data with Athena and QuickSight. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Deepen understanding of DevOps principles and practices on AWS. Explore hybrid cloud strategies and serverless integration. Learn cost optimization techniques for AWS projects. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Attend AWS Cloud Mastery Series #2 – DevOps on AWS 17/11/2025 17/11/2025 - Explore hybrid cloud strategies: connecting on-premises infrastructure to AWS with VPN \u0026amp; Direct Connect - Review AWS Storage Gateway use cases 18/11/2025 18/11/2025 - Learn about serverless architecture integration with DevOps workflows - Hands-on lab: deploy Lambda functions triggered by S3 events 19/11/2025 19/11/2025 AWS Lambda Workshop - Study cost optimization strategies for DevOps projects - Practice using AWS Cost Explorer and Budgets for multi-account setups 21/11/2025 21/11/2025 AWS Cost Management resources Week 11 Achievements: Attended the AWS Cloud Mastery Series #2 and strengthened knowledge of DevOps practices, CI/CD pipelines, container orchestration, and monitoring.\nGained understanding of hybrid cloud strategies and practical use of AWS Storage Gateway.\nLearned to integrate serverless architecture (Lambda) with DevOps workflows through hands-on labs.\nExplored cost optimization strategies using AWS Cost Explorer and Budgets for multi-account setups.\nSkills Gained:\nDevOps principles and DORA metrics understanding. CI/CD pipeline setup using AWS Developer Tools. Hybrid cloud architecture knowledge. Serverless integration with AWS Lambda. Cost monitoring and optimization using AWS tools. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Deepen understanding of AWS security best practices using the Well-Architected Security Framework. Explore hands-on techniques for identity, access, detection, infrastructure, and data protection. Gain practical skills in incident response automation and cloud security monitoring. Tasks to be carried out this week: Task Start Date Completion Date Reference Material - Review AWS Well-Architected Framework and Security Pillar concepts - Study IAM, MFA, SCP, Key Management Service (KMS), and Secrets Manager best practices 24/11/2025 24/11/2025 - Hands-on lab: implement secure IAM architecture, configure roles, policies, and permission boundaries 28/11/2025 28/11/2025 - Attend AWS Cloud Mastery Series #3 – AWS Well-Architected Security Pillar 29/11/2025 29/11/2025 Week 12 Achievements: Gained a comprehensive understanding of the AWS Well-Architected Security Pillar.\nLearned best practices for IAM, KMS, Secrets Manager, and policy enforcement.\nPracticed hands-on implementation of secure IAM architectures and permission management.\nStrengthened knowledge in continuous monitoring, network and infrastructure protection, and incident response automation.\nSkills Gained:\nAWS security best practices for identity, access, and data protection. Hands-on experience configuring IAM roles, policies, and permission boundaries. Knowledge of automated monitoring and incident response using AWS tools. Understanding of cloud security principles in real-world scenarios. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/5.3-infrastructure-setup/5.3.3-application-demonstration/","title":"Application demonstration","tags":[],"description":"","content":"Access application through load balancer Output after selecting a file and query Content is also stored to local storage Recovering the analysis of a file using hash "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/5.3-infrastructure-setup/","title":"Infrastructure Setup","tags":[],"description":"","content":"Infrastructure Setup In this section, you will configure and deploy the complete file analyzer infrastructure using Terraform. The process involves configuring variables, initializing Terraform, and deploying all AWS resources.\nWhat Will Be Created This Terraform configuration will provision:\nNetwork Infrastructure:\nVPC with custom CIDR block 2 Public subnets across availability zones 2 Private subnets across availability zones Internet Gateway and NAT Gateway Route tables and associations DNS configuration Compute Resources:\nApplication Load Balancer (ALB) with target groups Public Auto Scaling Group (min: 1, max: 3) Private Auto Scaling Group (min: 1, max: 3) Launch templates with user data scripts Health checks and scaling policies Supporting Services:\nDynamoDB table for application state Security groups for ALB, public ASG, and private ASG IAM roles and instance profiles for EC2 SSH key pair for instance access CI/CD Pipeline (Image Builder):\nEC2 Image Builder pipeline Custom AMI creation with application dependencies Automated image refresh workflow Terraform Module Structure The infrastructure is organized into modular components:\nnetworking.tf - VPC, subnets, gateways, routes security_groups.tf - Firewall rules for each tier services_load_balancer.tf - ALB configuration services_groups.tf - Auto Scaling Groups launch_template.tf - EC2 launch configurations dynamodb.tf - DynamoDB table setup image_builder.tf - AMI pipeline configuration dns.tf - DNS and routing configuration Estimated Time ⏱️ 10 minutes\nSteps Overview Configure Variables - Set AWS region, instance types, and capacity Initialize Terraform - Download AWS provider and validate configuration Deploy Infrastructure - Create all resources with single command 💡 All resources will be tagged with Environment: dev and ManagedBy: terraform for easy identification and cost tracking.\n⚠️ Note: The Image Builder pipeline may take additional time to create custom AMIs. The initial deployment will use the base AMI specified in variables.\nLet\u0026rsquo;s proceed with the configuration!\n"},{"uri":"https://hphuc2003919.github.io/FCJ-Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Introducing the new console experience for AWS WAF Blog 2 - AWS CIRT announces the launch of the Threat Technique Catalog for AWS Blog 3 - Introducing the AWS Security Champion Knowledge Path and digital badge "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/5.4-clean-up/","title":"Clean up","tags":[],"description":"","content":"Run terraform destroy to clean up "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in four events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00 - 17:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: The event began with opening remarks from government representatives and keynote addresses from AWS leaders, Techcombank and U2U Network CEOs. A panel discussion with executives from ELSA Corp, Nexttech Group, and TymeX explored strategies for navigating the GenAI revolution. In the afternoon, I attended Track 1: GenAI and Data, which covered topics such as building unified data foundations on AWS, GenAI adoption roadmaps, AI-driven development lifecycles (AI-DLC), securing generative AI applications, and the role of AI agents as productivity multipliers.\nOutcomes/Value Gained:\nStrengthened my understanding of how AWS supports scalable data infrastructure for AI and analytics workloads. Learned best practices for securing generative AI applications with AWS’s built-in measures. Gained insights into the AI-Driven Development Lifecycle (AI-DLC) and how AI can act as a co-developer. Understood the roadmap for GenAI adoption and its impact on shaping future innovation. Improved awareness of the evolving role of AI agents in enhancing productivity beyond traditional automation. Event 2 Event Name: AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 – 12:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: This event focused on practical applications of Generative AI using Amazon Bedrock, covering foundation model selection (Claude, Llama, Titan), prompt engineering techniques, Retrieval-Augmented Generation (RAG) architecture, Bedrock Agents for multi-step workflows, and guardrails for content safety. The session included a live demo showing how to build a GenAI chatbot using Bedrock services.\nOutcomes/Value Gained:\nStrengthened understanding of foundation models and how to choose the right one based on use cases. Gained practical skills in prompt engineering, CoT reasoning, and few-shot learning. Learned how to design and implement RAG systems and integrate knowledge bases effectively. Understood how to build AI agents with multi-step workflows using Bedrock Agents. Improved awareness of safety practices such as guardrails and content filtering. Developed hands-on experience through the live demo of constructing a GenAI chatbot. Event 3 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nDate \u0026amp; Time: 8:30 – 5:00, November 17, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: This event focused on modern DevOps culture, principles, and hands-on implementation of CI/CD, Infrastructure as Code, container orchestration, monitoring, and operational excellence on AWS. The morning covered DevOps mindset, DORA metrics, AWS CI/CD tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline), and Infrastructure as Code using CloudFormation and CDK. The afternoon explored containerization with Docker, Amazon ECR, ECS/EKS, App Runner, followed by monitoring and observability using CloudWatch and X-Ray. The event concluded with DevOps best practices, real-world case studies, and career guidance.\nOutcomes/Value Gained:\nGained a deeper understanding of DevOps culture, DORA metrics, and continuous improvement mindsets. Learned how to design and automate CI/CD pipelines using AWS developer tools. Enhanced skills in Infrastructure as Code with CloudFormation and CDK. Built practical knowledge of container services (ECR, ECS, EKS, App Runner) and microservices deployment strategies. Strengthened ability to monitor and troubleshoot distributed systems using CloudWatch and X-Ray. Understood best practices for incident management, automated testing, deployment strategies, and DevOps transformation. Event 4 Event Name: AWS Cloud Mastery Series #3 – AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 8:30 – 12:00, November 29, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: This event provided a deep dive into the security best practices recommended by AWS across the five core pillars: Identity \u0026amp; Access Management (IAM), Detection, Infrastructure Protection, Data Protection, and Incident Response. The workshop covered modern IAM architecture, multi-account governance, detection and monitoring strategies, network and workload protection, encryption and secrets management, and hands-on approaches to incident response. Real-world examples, demos, and discussions highlighted how organizations in Vietnam can strengthen their cloud security posture using AWS-native services and automation.\nOutcomes/Value Gained:\nDeep understanding of the AWS Security Pillar and how each of the five pillars contributes to a secure cloud architecture. Improved knowledge of modern IAM practices, including Identity Center, permission boundaries, SCPs, and avoiding long-term credentials. Practical experience in continuous monitoring, leveraging CloudTrail, GuardDuty, Security Hub, and EventBridge for detection and alert automation. Clear strategies for securing infrastructure, including VPC segmentation, Security Groups vs NACLs, and protection via WAF, Shield, and Network Firewall. Stronger grasp of data protection principles: KMS key management, encryption patterns, secrets rotation, and data classification. Hands-on insight into incident response workflows, from detecting compromised keys to isolating infected workloads and automating remediation using Lambda and Step Functions. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploying To AWS Cloud Using Terraform Overview This lab provides a guided, hands-on introduction to provisioning cloud infrastructure on Amazon Web Services (AWS) using Terraform. Participants learn how to define infrastructure as code, manage configuration state, and execute automated workflows to create, update, and destroy cloud resources in a controlled and repeatable way.\nContent Workshop overview Prerequiste Initialize the infrastructure How to destroy initialized infrastructure "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey (FCJ) from 8/9/2025 to 12/12/2025, I had the opportunity to apply my academic knowledge to a real-world environment while working on the File Analyzer project, a cloud-based multi-service file analysis system. Through this experience, I gained practical exposure to real-world software development, team collaboration, and cloud infrastructure.\nI tried my best to approach tasks responsibly and maintain professionalism. While I contributed consistently, I also recognized areas where I need further improvement, especially in technical knowledge, communication, and problem-solving skills.\nTo objectively reflect on my internship performance, I have evaluated myself on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ☐ ✅ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ☐ ✅ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ☐ ✅ Reflection Strengths: Demonstrated responsibility and discipline by completing tasks on time and adhering to team rules. Maintained professional conduct and respect towards colleagues. Contributed consistently to team efforts and project workflow. Areas for Improvement: Strengthen technical knowledge and practical skills to perform tasks more confidently. Improve communication skills to explain ideas clearly and effectively in team discussions. Enhance problem-solving skills and creativity when approaching complex challenges. Develop a more proactive mindset to identify opportunities and contribute beyond assigned tasks. "},{"uri":"https://hphuc2003919.github.io/FCJ-Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Experience Joining the First Cloud Journey program and working on a project with other members during the First Cloud Journey program has been an eye-opening experience. I enjoyed being part of a real-world project that combines cloud services, backend systems, and security-focused file analysis. It gave me the chance to see how different components interact in a distributed system and how important careful planning is for scalability and reliability.\nCollaboration and Mentorship I appreciated the support and guidance from the admin team and team members. They were always ready to explain concepts, answer questions, and give constructive feedback. I especially valued that I was encouraged to try solutions on my own, which helped me develop problem-solving skills and confidence in my abilities.\nSkills and Knowledge Gained Through this project, I learned:\nHow to design and implement a modular multi-service architecture for real-world applications. Practical use of cloud deployment and infrastructure-as-code (Terraform) for creating scalable and reproducible environments. Importance of secure file handling and data persistence in a distributed system. How to integrate frontend, backend, and database services effectively while maintaining clean code structure. Enhanced team collaboration and professional communication, especially when coordinating tasks and reviewing code. Most Satisfying Aspect The most rewarding part was seeing my contributions directly impact the workflow of the system, whether it was improving a service interaction, handling file validation, or ensuring proper database integration. Successfully solving challenges independently gave me a sense of accomplishment.\nSuggestions for Improvement More structured peer review sessions or code walkthroughs could help interns learn from each other and improve code quality. Occasional team-building or knowledge-sharing sessions could strengthen team relationships and foster cross-learning. Overall Reflection This internship and project experience taught me not only technical skills but also the importance of collaboration, feedback, and structured problem-solving. It has reinforced my interest in cloud development and secure software design, and I feel better prepared for future projects in these areas.\n"},{"uri":"https://hphuc2003919.github.io/FCJ-Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://hphuc2003919.github.io/FCJ-Report/tags/","title":"Tags","tags":[],"description":"","content":""}]